{
  "baseline": {
    "metrics": {
      "mae_overall": 23.29,
      "rmse_overall": 30.6,
      "r2_overall": 0.752,
      "mae_day1": 14.91,
      "mae_day2": 20.98,
      "mae_day3": 23.79,
      "mae_day4": 25.07,
      "mae_day5": 25.79,
      "mae_day6": 26.13,
      "mae_day7": 26.33,
      "rmse_day1": 20.02,
      "r2_day1": 0.894,
      "r2_day7": 0.6961
    },
    "config": {
      "features": 12,
      "hidden_dim": 256,
      "num_layers": 2,
      "architecture": "GRU with attention",
      "learning_rate": 0.001,
      "batch_size": 32,
      "epochs": 18
    }
  },
  "simplified_gru_128h_1l": {
    "metrics": {
      "mae_overall": 23.68,
      "rmse_overall": 30.99,
      "r2_overall": 0.7456,
      "mae_day1": 14.67,
      "mae_day2": 21.22,
      "mae_day3": 24.02,
      "mae_day4": 25.46,
      "mae_day5": 26.2,
      "mae_day6": 26.91,
      "mae_day7": 27.24,
      "rmse_day1": 19.52,
      "rmse_day7": 34.36,
      "r2_day1": 0.8992,
      "r2_day7": 0.6863
    },
    "config": {
      "features": 12,
      "hidden_dim": 128,
      "num_layers": 1,
      "architecture": "GRU with attention (simplified)",
      "learning_rate": 0.001,
      "batch_size": 32,
      "epochs_trained": 21,
      "best_epoch": 6,
      "total_parameters": 187137,
      "seed": 42,
      "notes": "Simplified architecture to reduce overfitting. Trained longer (21 epochs) but still overfits after epoch 6."
    }
  },
  "standard_gru_256h_2l_seed42": {
    "metrics": {
      "mae_overall": 23.35,
      "rmse_overall": 30.96,
      "r2_overall": 0.746,
      "mae_day1": 15.03,
      "mae_day2": 20.71,
      "mae_day3": 23.51,
      "mae_day4": 25.07,
      "mae_day5": 26.0,
      "mae_day6": 26.49,
      "mae_day7": 26.68,
      "rmse_day1": 20.07,
      "rmse_day7": 34.62,
      "r2_day1": 0.8935,
      "r2_day7": 0.6815
    },
    "config": {
      "features": 12,
      "hidden_dim": 256,
      "num_layers": 2,
      "architecture": "GRU with attention",
      "learning_rate": 0.001,
      "batch_size": 32,
      "epochs_trained": 17,
      "best_epoch": 2,
      "total_parameters": 1524225,
      "seed": 42,
      "notes": "Standard architecture with reproducible seed. Nearly identical to baseline. Severe overfitting - best at epoch 2, then diverges (train=0.030, val=0.308 at epoch 17)."
    }
  },
  "gru_no_tg_11feat": {
    "metrics": {
      "mae_overall": 23.19,
      "rmse_overall": 30.32,
      "r2_overall": 0.7564,
      "mae_day1": 15.1,
      "mae_day2": 21.2,
      "mae_day3": 23.88,
      "mae_day4": 24.91,
      "mae_day5": 25.55,
      "mae_day6": 25.83,
      "mae_day7": 25.9,
      "rmse_day1": 19.77,
      "rmse_day7": 33.15,
      "r2_day1": 0.8966,
      "r2_day7": 0.708
    },
    "config": {
      "features": 11,
      "feature_list": "TN, TX, RR, SS, HU, FG, FX, CC, SD, DAY_SIN, DAY_COS",
      "excluded": "TG (target), PP, QQ",
      "hidden_dim": 256,
      "num_layers": 2,
      "architecture": "GRU with attention",
      "learning_rate": 0.001,
      "batch_size": 32,
      "epochs_trained": 17,
      "best_epoch": 2,
      "total_parameters": 1523457,
      "seed": 42,
      "notes": "BEST MODEL SO FAR. Removed TG from features to eliminate potential data leakage. Surprisingly IMPROVED results vs including TG. Still overfits at epoch 2 (train=0.032, val=0.307 at end). Beats baseline by 0.10 MAE."
    }
  },
  "cnn_1d_11feat": {
    "metrics": {
      "mae_overall": 23.69,
      "rmse_overall": 30.82,
      "r2_overall": 0.7484,
      "mae_day1": 19.22,
      "mae_day2": 22.1,
      "mae_day3": 23.82,
      "mae_day4": 24.5,
      "mae_day5": 25.21,
      "mae_day6": 25.51,
      "mae_day7": 25.46,
      "rmse_day1": 24.67,
      "rmse_day7": 32.7,
      "r2_day1": 0.8391,
      "r2_day7": 0.7159
    },
    "config": {
      "features": 11,
      "feature_list": "TN, TX, RR, SS, HU, FG, FX, CC, SD, DAY_SIN, DAY_COS",
      "excluded": "TG (target), PP, QQ",
      "architecture": "1D CNN",
      "conv_layers": "3 layers (64, 128, 128 channels)",
      "kernel_sizes": "3, 3, 5",
      "dense_layers": "256 -> 128 -> 7",
      "dropout": 0.3,
      "learning_rate": 0.001,
      "batch_size": 32,
      "epochs_trained": 29,
      "best_epoch": 14,
      "total_parameters": 241287,
      "seed": 42,
      "notes": "WORSE than GRU. Trained more stably (29 epochs, small train/val gap of 0.042) but CATASTROPHIC at 1-day forecasts (19.22 vs GRU 15.10, +27% worse). MaxPooling destroyed temporal precision. Slightly better at 7-day (+1.7%). Overall: 2.2% worse than GRU."
    }
  },
  "cnn_with_tg_12feat": {
    "metrics": {
      "mae_overall": 24.01,
      "rmse_overall": 30.79,
      "r2_overall": 0.7489,
      "mae_day1": 18.2,
      "mae_day2": 22.07,
      "mae_day3": 24.21,
      "mae_day4": 25.14,
      "mae_day5": 25.88,
      "mae_day6": 26.21,
      "mae_day7": 26.36,
      "rmse_day1": 23.2,
      "rmse_day7": 33.19,
      "r2_day1": 0.8576,
      "r2_day7": 0.7072
    },
    "config": {
      "features": 12,
      "feature_list": "TG, TN, TX, RR, SS, HU, FG, FX, CC, SD, DAY_SIN, DAY_COS",
      "excluded": "PP, QQ",
      "architecture": "1D CNN",
      "conv_layers": "3 layers (64, 128, 128 channels)",
      "kernel_sizes": "3, 3, 5",
      "dense_layers": "256 -> 128 -> 7",
      "dropout": 0.3,
      "learning_rate": 0.001,
      "batch_size": 32,
      "epochs_trained": 22,
      "best_epoch": 7,
      "total_parameters": 241479,
      "seed": 42,
      "notes": "WORST OVERALL MODEL (24.01 MAE). Adding TG helped 1-day forecasts (19.22\u219218.20, +5% better) but hurt 7-day (25.46\u219226.36, +3.5% worse). Still 20.5% worse than GRU at 1-day forecasts. CNN architecture fundamentally unsuited for this task - maxpooling destroys temporal precision."
    }
  }
}